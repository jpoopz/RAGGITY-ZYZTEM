# RAG System Configuration
# Values here override defaults; environment variables override this file

# LLM Provider Configuration
provider: ollama  # ollama or openai
model_name: llama3.2
embedding_model: nomic-embed-text

# Ollama Settings
ollama_host: http://localhost:11434

# OpenAI Settings (only used if provider=openai)
# openai_api_key: sk-...  # Better to use env var OPENAI_API_KEY
openai_model: gpt-4o-mini
openai_embedding_model: text-embedding-3-small

# Generation Settings
temperature: 0.3
timeout: 120

# Vector Store Configuration
vector_store: faiss  # faiss or chroma
vector_dir: vector_store
chroma_dir: .chromadb

# Hybrid Mode - Delegate queries to cloud when available
hybrid_mode: true  # true or false (1 or 0)

# Data Directories
data_dir: data

# Academic Research Module
academic:
  enabled: true
  providers:
    openalex:
      enabled: true
      polite_email: "julian.poopat@gmail.com"
    crossref:
      enabled: true
      polite_email: "julian.poopat@gmail.com"
    arxiv:
      enabled: true
    semanticscholar:
      enabled: false
      api_key: ""  # Optional: set via env var S2_API_KEY
    unpaywall:
      enabled: true
      email: "julian.poopat@gmail.com"
  citations:
    style: "harvard-cite-them-right"
    locale: "en-GB"
  retrieval:
    dense_model: "sentence-transformers/all-MiniLM-L6-v2"
    bm25_impl: "rank_bm25"  # or "bm25s" for faster alternative
    rrf_k: 60  # Reciprocal Rank Fusion parameter
    top_k: 12  # Number of papers to retrieve
  grobid:
    url: "http://localhost:8070"
    enabled: true

